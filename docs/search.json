[
  {
    "objectID": "index.html#training-1",
    "href": "index.html#training-1",
    "title": "LLMs on Polaris",
    "section": "Training",
    "text": "Training\n\nLet‚Äôs assume our output vocab has only six words: \nHow do you compare two probability distributions?\n\n[cross_entropy, KL-Divergence, ...]\n\nOne-hot encoding of the word ‚Äúam‚Äù:"
  },
  {
    "objectID": "index.html#training-loss-function",
    "href": "index.html#training-loss-function",
    "title": "LLMs on Polaris",
    "section": "Training: Loss Function",
    "text": "Training: Loss Function\n\nOutput:\n\n\n\n\n\n\n\n\n\nMontgomery, Samuel. 2023. ‚ÄúMastering Language Models.‚Äù Medium. Towards Data Science. https://towardsdatascience.com/mastering-language-models-32e1d891511a.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. ‚ÄúAttention Is All You Need.‚Äù https://arxiv.org/abs/1706.03762.\n\n\nYang, Jingfeng, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Bing Yin, and Xia Hu. 2023. ‚ÄúHarnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond.‚Äù https://arxiv.org/abs/2304.13712.\n\n\nYao, Shunyu, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. ‚ÄúTree of Thoughts: Deliberate Problem Solving with Large Language Models.‚Äù https://arxiv.org/abs/2305.10601."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "LLMs on Polaris",
    "section": "",
    "text": "October 10 ‚Äì 12, 2023 \\hspace{5pt}  \n\nALCF Hands-on\n HPC Workshop"
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "LLMs on Polaris",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n Hannibal046/Awesome-LLM‚Ü©Ô∏é\nFigure from The Illustrated Transformer‚Ü©Ô∏é\nOn-top of the base conda environment (--system-site-packages)‚Ü©Ô∏é\nü§ó Model Parallelism‚Ü©Ô∏é\nü§ó Model Parallelism‚Ü©Ô∏é\nBlog Post‚Ü©Ô∏é\nEfficient Large-Scale Language Model Training on GPU Clusters‚Ü©Ô∏é\nThe described experiments were performed on 4 NVIDIA DGX A100-40GB nodes, all using TPSIZE=32[^tpsize], connected through 8 HDR InfiniBand (200Gb/s per HDR).‚Ü©Ô∏é‚Ü©Ô∏é\nthroughput/TFLOPS‚Ü©Ô∏é"
  }
]