<!DOCTYPE html>
<html lang="en"><head>
<link href="././favicon.svg" rel="icon" type="image/svg+xml">
<script src="site_libs/quarto-html/tabby.min.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-contrib/fontawesome6-0.1.0/all.css" rel="stylesheet">
<link href="site_libs/quarto-contrib/fontawesome6-0.1.0/latex-fontsize.css" rel="stylesheet"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.3.450">

  <meta name="author" content="Sam Foreman">
  <meta name="dcterms.date" content="2023-10-11">
  <title>LLMs on Polaris</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
      }
    pre.numberSource { margin-left: 3em;  padding-left: 4px; }
    div.sourceCode
      { color: #383a42;  }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #383a42; } /* Normal */
    code span.al { color: #95da4c; background-color: #4d1f24; font-weight: bold; } /* Alert */
    code span.an { color: #50a14f; } /* Annotation */
    code span.at { color: #a626a4; } /* Attribute */
    code span.bn { color: #986801; } /* BaseN */
    code span.bu { color: #a626a4; } /* BuiltIn */
    code span.cf { color: #a626a4; } /* ControlFlow */
    code span.ch { color: #50a14f; } /* Char */
    code span.cn { color: #986801; } /* Constant */
    code span.co { color: #a0a1a7; font-style: italic; } /* Comment */
    code span.cv { color: #e45649; font-style: italic; } /* CommentVar */
    code span.do { color: #e45649; } /* Documentation */
    code span.dt { color: #a626a4; } /* DataType */
    code span.dv { color: #986801; } /* DecVal */
    code span.er { color: #f44747; text-decoration: underline; } /* Error */
    code span.ex { color: #4078f2; font-weight: bold; } /* Extension */
    code span.fl { color: #986801; } /* Float */
    code span.fu { color: #4078f2; } /* Function */
    code span.im { color: #50a14f; } /* Import */
    code span.in { color: #c45b00; } /* Information */
    code span.kw { color: #a626a4; } /* Keyword */
    code span.op { color: #a626a4; } /* Operator */
    code span.ot { color: #27ae60; } /* Other */
    code span.pp { color: #a626a4; } /* Preprocessor */
    code span.re { color: #2980b9; background-color: #153042; } /* RegionMarker */
    code span.sc { color: #0184bc; } /* SpecialChar */
    code span.ss { color: #da4453; } /* SpecialString */
    code span.st { color: #50a14f; } /* String */
    code span.va { color: #e45649; } /* Variable */
    code span.vs { color: #da4453; } /* VerbatimString */
    code span.wa { color: #da4453; } /* Warning */
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  <link rel="stylesheet" href="site_libs/revealjs/dist/theme/quarto.css">
  <link rel="stylesheet" href="css/default.css">
  <link rel="stylesheet" href="css/callouts-html.css">
  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">
  <link href="site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    margin-bottom: 0.5rem;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
<meta name="twitter:title" content="LLMs on Polaris">
<meta name="twitter:image" content="https://github.com/saforem2/llm-lunch-talk/blob/main/docs/assets/thumbnail.png?raw=true">
<meta name="twitter:creator" content="@saforem2">
<meta name="twitter:site" content="@saforem2">
<meta name="twitter:card" content="summary_large_image">
<meta name="citation_title" content="LLMs at ALCF">
<meta name="citation_author" content="Sam Foreman">
<meta name="citation_publication_date" content="2023-10-11">
<meta name="citation_cover_date" content="2023-10-11">
<meta name="citation_year" content="2023">
<meta name="citation_online_date" content="2023-10-11">
<meta name="citation_fulltext_html_url" content="https://saforem2.github.io/llm-lunch-talk">
<meta name="citation_language" content="en">
<meta name="citation_reference" content="citation_title=Progress on (g-2)_\mu from lattice QCD;,citation_author=Hartmut Wittig;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2306.04165;">
<meta name="citation_reference" content="citation_title=Hybrid Monte Carlo;,citation_author=S. Duane;,citation_author=A. D. Kennedy;,citation_author=B. J. Pendleton;,citation_author=D. Roweth;,citation_publication_date=1987;,citation_cover_date=1987;,citation_year=1987;,citation_doi=10.1016/0370-2693(87)91197-X;,citation_volume=195;,citation_journal_title=Phys. Lett. B;">
<meta name="citation_reference" content="citation_title=Snowmass 2021 Computational Frontier CompF03 Topical Group Report: Machine Learning;,citation_author=Phiala Shanahan;,citation_author=others;,citation_publication_date=2022-09;,citation_cover_date=2022-09;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2209.07559;">
<meta name="citation_reference" content="citation_title=Applications of Machine Learning to Lattice Quantum Field Theory;,citation_author=Denis Boyda;,citation_author=others;,citation_publication_date=2022-02;,citation_cover_date=2022-02;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2202.05838;,citation_conference_title=Snowmass 2021;">
<meta name="citation_reference" content="citation_title=LeapfrogLayers: A Trainable Framework for Effective Topological Sampling;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_publication_date=2022-05;,citation_cover_date=2022-05;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2112.01582;,citation_doi=10.22323/1.396.0508;,citation_volume=LATTICE2021;,citation_journal_title=PoS;">
<meta name="citation_reference" content="citation_title=HMC with Normalizing Flows;,citation_author=Sam Foreman;,citation_author=Taku Izubuchi;,citation_author=Luchang Jin;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_author=Akio Tomiya;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2112.01586;,citation_doi=10.22323/1.396.0073;,citation_volume=LATTICE2021;,citation_journal_title=PoS;">
<meta name="citation_reference" content="citation_title=Deep Learning Hamiltonian Monte Carlo;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_publication_date=2021-05;,citation_cover_date=2021-05;,citation_year=2021;,citation_fulltext_html_url=https://arxiv.org/abs/2105.03418;,citation_conference_title=9th International Conference on Learning Representations;">
<meta name="citation_reference" content="citation_title=Mastering language models;,citation_author=Samuel Montgomery;,citation_publication_date=2023-10;,citation_cover_date=2023-10;,citation_year=2023;,citation_fulltext_html_url=https://towardsdatascience.com/mastering-language-models-32e1d891511a;,citation_journal_title=Medium;,citation_publisher=Towards Data Science;">
<meta name="citation_reference" content="citation_title=Harnessing the power of LLMs in practice: A survey on ChatGPT and beyond;,citation_author=Jingfeng Yang;,citation_author=Hongye Jin;,citation_author=Ruixiang Tang;,citation_author=Xiaotian Han;,citation_author=Qizhang Feng;,citation_author=Haoming Jiang;,citation_author=Bing Yin;,citation_author=Xia Hu;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2304.13712;">
<meta name="citation_reference" content="citation_title=Training tips for the transformer model;,citation_author=Martin Popel;,citation_author=Ondřej Bojar;,citation_publication_date=2018-04;,citation_cover_date=2018-04;,citation_year=2018;,citation_fulltext_html_url=https://doi.org/10.2478%2Fpralin-2018-0002;,citation_issue=1;,citation_doi=10.2478/pralin-2018-0002;,citation_volume=110;,citation_journal_title=The Prague Bulletin of Mathematical Linguistics;,citation_publisher=Charles University in Prague, Karolinum Press;">
<meta name="citation_reference" content="citation_title=Attention is all you need;,citation_author=Ashish Vaswani;,citation_author=Noam Shazeer;,citation_author=Niki Parmar;,citation_author=Jakob Uszkoreit;,citation_author=Llion Jones;,citation_author=Aidan N. Gomez;,citation_author=Lukasz Kaiser;,citation_author=Illia Polosukhin;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_fulltext_html_url=https://arxiv.org/abs/1706.03762;">
<meta name="citation_reference" content="citation_title=Tree of thoughts: Deliberate problem solving with large language models;,citation_author=Shunyu Yao;,citation_author=Dian Yu;,citation_author=Jeffrey Zhao;,citation_author=Izhak Shafran;,citation_author=Thomas L. Griffiths;,citation_author=Yuan Cao;,citation_author=Karthik Narasimhan;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2305.10601;">
<meta name="citation_reference" content="citation_title=GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics;,citation_abstract=We seek to transform how new and emergent variants of pandemiccausing viruses, specifically SARS-CoV-2, are identified and classified. By adapting large language models (LLMs) for genomic data, we build genome-scale language models (GenSLMs) which can learn the evolutionary landscape of SARS-CoV-2 genomes. By pretraining on over 110 million prokaryotic gene sequences and finetuning a SARS-CoV-2-specific model on 1.5 million genomes, we show that GenSLMs can accurately and rapidly identify variants of concern. Thus, to our knowledge, GenSLMs represents one of the first whole genome scale foundation models which can generalize to other prediction tasks. We demonstrate scaling of GenSLMs on GPU-based supercomputers and AI-hardware accelerators utilizing 1.63 Zettaflops in training runs with a sustained performance of 121 PFLOPS in mixed precision and peak of 850 PFLOPS. We present initial scientific insights from examining GenSLMs in tracking evolutionary dynamics of SARS-CoV-2, paving the path to realizing this on large biological data.Competing Interest StatementThe authors have declared no competing interest.;,citation_author=Maxim Zvyagin;,citation_author=Alexander Brace;,citation_author=Kyle Hippe;,citation_author=Yuntian Deng;,citation_author=Bin Zhang;,citation_author=Cindy Orozco Bohorquez;,citation_author=Austin Clyde;,citation_author=Bharat Kale;,citation_author=Danilo Perez-Rivera;,citation_author=Heng Ma;,citation_author=Carla M. Mann;,citation_author=Michael Irvin;,citation_author=J. Gregory Pauloski;,citation_author=Logan Ward;,citation_author=Valerie Hayot-Sasson;,citation_author=Murali Emani;,citation_author=Sam Foreman;,citation_author=Zhen Xie;,citation_author=Diangen Lin;,citation_author=Maulik Shukla;,citation_author=Weili Nie;,citation_author=Josh Romero;,citation_author=Christian Dallago;,citation_author=Arash Vahdat;,citation_author=Chaowei Xiao;,citation_author=Thomas Gibbs;,citation_author=Ian Foster;,citation_author=James J. Davis;,citation_author=Michael E. Papka;,citation_author=Thomas Brettin;,citation_author=Rick Stevens;,citation_author=Anima Anandkumar;,citation_author=Venkatram Vishwanath;,citation_author=Arvind Ramanathan;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://www.biorxiv.org/content/early/2022/11/23/2022.10.10.511571;,citation_doi=10.1101/2022.10.10.511571;,citation_journal_title=bioRxiv;,citation_publisher=Cold Spring Harbor Laboratory;">
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">LLMs on Polaris</h1>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
<a href="https://samforeman.me">Sam Foreman</a> <a href="https://orcid.org/0000-0002-9981-0876" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a>
</div>
<div class="quarto-title-author-email">
<a href="mailto:foremans@anl.gov">foremans@anl.gov</a>
</div>
        <p class="quarto-title-affiliation">
            <a href="https://alcf.anl.gov/about/people/sam-foreman">
            Argonne National Laboratory
            </a>
          </p>
    </div>
</div>

  <p class="date">2023-10-11</p>
</section>
<section id="section" class="title-slide slide level1 centeredslide center" data-background-image="./assets/image2.png" loading="lazy">
<h1></h1>
<!-- # {.centerdslide background-image="https://www.alcf.anl.gov/sites/default/files/2023-08/ALCF-HandsOnHPCWksp-LL.png?itok=6qi5GY6y" height="80%"} -->
<!-- # {.centeredslide background-image="./assets/massstar_science_highlights_2017_01.png" loading="lazy"} -->
<!-- # {.centeredslide background-image="./assets/p62_cover-edit_CMYK.jpg" loading="lazy"} -->
<!-- # {.centeredslide background-image="./assets/tribology_cover_test_image_06m.png" loading="lazy"} -->
<!-- # {.centeredslide background-image="./assets/6120702714_c9a4cf5d78_o.jpg" loading="lazy"} -->
<!-- # {.centeredslide background-image="./assets/ccm_s23-50_Ye_03B.png" loading="lazy"} -->
<div style="background-color: rgba(8, 42, 123, 0.7); border-radius: 10px; text-align:left; padding: 1.5rem; margin-left: auto; margin-right: auto; line-height: 1.5em!important;">
<div style="display:flex;">
<p><span style="font-size: 1.75em; font-weight: 600; border-bottom: 1px solid white; color: #F8F8F8">October 10 – 12, 2023 <span class="math inline">\hspace{5pt}</span> <i class="fa-solid fa-shapes" aria-label="shapes"></i></span> <span style="display:inline-block;"><img data-src="./assets/anl_logo.svg"></span></p>
</div>
<p><span style="font-size: 3.0em; font-weight: 700; color: white">ALCF Hands-on</span><br>
<br> <span style="font-size: 3.0em; font-weight: 700; color: #FFFFFF">HPC Workshop</span></p>
</div>
</section>

<section id="status-of-large-language-models" class="title-slide slide level1 center">
<h1>Status of Large Language Models</h1>
<div id="fig-llms" class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="https://github.com/Hannibal046/Awesome-LLM/raw/main/resources/image8.gif"></p>
<figcaption>Figure&nbsp;1: Large Language Models have (LLM)s have taken the <del>NLP community</del> <strong>world</strong> by storm<sup>1</sup></figcaption>
</figure>
</div>
<aside><ol class="aside-footnotes"><li id="fn1"><p><a href="https://github.com/Hannibal046/Awesome-LLM"><i class="fa-brands fa-github" aria-label="github"></i> <code>Hannibal046/Awesome-LLM</code></a></p></li></ol></aside></section>


<section id="emergent-abilities" class="title-slide slide level1 center" data-background-color="#FBFBFD">
<h1>Emergent Abilities</h1>
<div width="66%" style="text-align: center;">
<p><img src="./assets/emergent-abilities.gif" height="75%"></p>
<p><a href="https://arxiv.org/abs/2206.07682">Emergent abilities of Large Language Models</a> <span class="citation" data-cites="yao2023tree">Yao et al. (<a href="#/references" role="doc-biblioref" onclick="">2023</a>)</span></p>
</div>
</section>

<section id="training-llms" class="title-slide slide level1 center">
<h1>Training LLMs</h1>
<div class="quarto-layout-panel">
<div class="quarto-layout-row quarto-layout-valign-bottom">
<div id="fig-evolution" class="quarto-figure quarto-figure-center" style="flex-basis: 55.0%;justify-content: center;">
<figure>
<p><img data-src="https://github.com/Mooler0410/LLMsPracticalGuide/raw/main/imgs/survey-gif-test.gif"></p>
<figcaption>Figure&nbsp;2: Visualization from <span class="citation" data-cites="yang2023harnessing">Yang et al. (<a href="#/references" role="doc-biblioref" onclick="">2023</a>)</span></figcaption>
</figure>
</div>
<div class="quarto-layout-cell" style="flex-basis: 45.0%;justify-content: center;">
<p><img data-src="./assets/it_hungers.jpeg"></p>
</div>
</div>
</div>
</section>

<section id="recent-work-2017-now" class="title-slide slide level1 scrollable center" style="max-height: 90%; height: 100%;">
<h1>Recent Work (2017 – Now)</h1>
<details closed="">
<summary>
<b>Recent Work</b>
</summary>
<div class="table-responsive" width="100%" style="max-height: 550px!important; font-size: 0.7rem;" data-quarto-disable-processing="true">
<div id="tbl-papers">
<table class="table-striped table-hover">
<caption>Table&nbsp;1: Papers, 2017–*</caption>
<thead>
<tr class="header">
<th style="text-align: center;">Date</th>
<th style="text-align: center;">keywords</th>
<th style="text-align: center;">Institute</th>
<th style="text-align: left;">Paper</th>
<th style="text-align: center;">Publication</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">2017-06</td>
<td style="text-align: center;">Transformers</td>
<td style="text-align: center;">Google</td>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/1706.03762.pdf">Attention Is All You Need</a></td>
<td style="text-align: center;">NeurIPS<br> <img data-src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F204e3073870fae3d05bcbc2f6a8e263d9b72e776%3Ffields%3DcitationCount&amp;query=%24.citationCount&amp;label=citation" alt="Dynamic JSON Badge"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2018-06</td>
<td style="text-align: center;">GPT 1.0</td>
<td style="text-align: center;">OpenAI</td>
<td style="text-align: left;"><a href="https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf">Improving Language Understanding by Generative Pre-Training</a></td>
<td style="text-align: center;"><img data-src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fcd18800a0fe0b668a1cc19f2ec95b5003d0a5035%3Ffields%3DcitationCount&amp;query=%24.citationCount&amp;label=citation" alt="Dynamic JSON Badge"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2018-10</td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">Google</td>
<td style="text-align: left;"><a href="https://aclanthology.org/N19-1423.pdf">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></td>
<td style="text-align: center;">NAACL <br><img data-src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fdf2b0e26d0599ce3e70df8a9da02e51594e0e992%3Ffields%3DcitationCount&amp;query=%24.citationCount&amp;label=citation" alt="Dynamic JSON Badge"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2019-02</td>
<td style="text-align: center;">GPT 2.0</td>
<td style="text-align: center;">OpenAI</td>
<td style="text-align: left;"><a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">Language Models are Unsupervised Multitask Learners</a></td>
<td style="text-align: center;"><img data-src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F9405cc0d6169988371b2755e573cc28650d14dfe%3Ffields%3DcitationCount&amp;query=%24.citationCount&amp;label=citation" alt="Dynamic JSON Badge"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2019-09</td>
<td style="text-align: center;">Megatron-LM</td>
<td style="text-align: center;">NVIDIA</td>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/1909.08053.pdf">Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism</a></td>
<td style="text-align: center;"><img data-src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F8323c591e119eb09b28b29fd6c7bc76bd889df7a%3Ffields%3DcitationCount&amp;query=%24.citationCount&amp;label=citation" alt="Dynamic JSON Badge"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2019-10</td>
<td style="text-align: center;">T5</td>
<td style="text-align: center;">Google</td>
<td style="text-align: left;"><a href="https://jmlr.org/papers/v21/20-074.html">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a></td>
<td style="text-align: center;">JMLR<br> <img data-src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F3cfb319689f06bf04c2e28399361f414ca32c4b3%3Ffields%3DcitationCount&amp;query=%24.citationCount&amp;label=citation" alt="Dynamic JSON Badge"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2019-10</td>
<td style="text-align: center;">ZeRO</td>
<td style="text-align: center;">Microsoft</td>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/1910.02054.pdf">ZeRO: Memory Optimizations Toward Training Trillion Parameter Models</a></td>
<td style="text-align: center;">SC<br> <img data-src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F00c957711b12468cb38424caccdf5291bb354033%3Ffields%3DcitationCount&amp;query=%24.citationCount&amp;label=citation" alt="Dynamic JSON Badge"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2020-01</td>
<td style="text-align: center;">Scaling Law</td>
<td style="text-align: center;">OpenAI</td>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2001.08361.pdf">Scaling Laws for Neural Language Models</a></td>
<td style="text-align: center;"><img data-src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fe6c561d02500b2596a230b341a8eb8b921ca5bf2%3Ffields%3DcitationCount&amp;query=%24.citationCount&amp;label=citation" alt="Dynamic JSON Badge"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2020-05</td>
<td style="text-align: center;">GPT 3.0</td>
<td style="text-align: center;">OpenAI</td>
<td style="text-align: left;"><a href="https://papers.nips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf">Language models are few-shot learners</a></td>
<td style="text-align: center;">NeurIPS <br> <img data-src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F6b85b63579a916f705a8e10a49bd8d849d91b1fc%3Ffields%3DcitationCount&amp;query=%24.citationCount&amp;label=citation" alt="Dynamic JSON Badge"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2021-01</td>
<td style="text-align: center;">Switch Transformers</td>
<td style="text-align: center;">Google</td>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2101.03961.pdf">Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity</a></td>
<td style="text-align: center;">JMLR<br> <img data-src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Ffdacf2a732f55befdc410ea927091cad3b791f13%3Ffields%3DcitationCount&amp;query=%24.citationCount&amp;label=citation" alt="Dynamic JSON Badge"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2021-08</td>
<td style="text-align: center;">Codex</td>
<td style="text-align: center;">OpenAI</td>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2107.03374.pdf">Evaluating Large Language Models Trained on Code</a></td>
<td style="text-align: center;"><img data-src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Facbdbf49f9bc3f151b93d9ca9a06009f4f6eb269%3Ffields%3DcitationCount&amp;query=%24.citationCount&amp;label=citation" alt="Dynamic JSON Badge"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2021-08</td>
<td style="text-align: center;">Foundation Models</td>
<td style="text-align: center;">Stanford</td>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2108.07258.pdf">On the Opportunities and Risks of Foundation Models</a></td>
<td style="text-align: center;"><img data-src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F4f68e07c6c3173480053fd52391851d6f80d651b%3Ffields%3DcitationCount&amp;query=%24.citationCount&amp;label=citation" alt="Dynamic JSON Badge"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2021-09</td>
<td style="text-align: center;">FLAN</td>
<td style="text-align: center;">Google</td>
<td style="text-align: left;"><a href="https://openreview.net/forum?id=gEZrGCozdqR">Finetuned Language Models are Zero-Shot Learners</a></td>
<td style="text-align: center;">ICLR <br><img data-src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fff0b2681d7b05e16c46dfb71d980cc2f605907cd%3Ffields%3DcitationCount&amp;query=%24.citationCount&amp;label=citation" alt="Dynamic JSON Badge"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2021-10</td>
<td style="text-align: center;">T0</td>
<td style="text-align: center;">HuggingFace et al.</td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2110.08207">Multitask Prompted Training Enables Zero-Shot Task Generalization</a></td>
<td style="text-align: center;">ICLR <br><img data-src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F17dd3555fd1ccf1141cf984347fa1b3fd6b009ca%3Ffields%3DcitationCount&amp;query=%24.citationCount&amp;label=citation" alt="Dynamic JSON Badge"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2021-12</td>
<td style="text-align: center;">GLaM</td>
<td style="text-align: center;">Google</td>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2112.06905.pdf">GLaM: Efficient Scaling of Language Models with Mixture-of-Experts</a></td>
<td style="text-align: center;">ICML<br> <img data-src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F80d0116d77beeded0c23cf48946d9d10d4faee14%3Ffields%3DcitationCount&amp;query=%24.citationCount&amp;label=citation" alt="Dynamic JSON Badge"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2021-12</td>
<td style="text-align: center;">WebGPT</td>
<td style="text-align: center;">OpenAI</td>
<td style="text-align: left;"><a href="https://www.semanticscholar.org/paper/WebGPT%3A-Browser-assisted-question-answering-with-Nakano-Hilton/2f3efe44083af91cef562c1a3451eee2f8601d22">WebGPT: Browser-assisted question-answering with human feedback</a></td>
<td style="text-align: center;"><img data-src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F2f3efe44083af91cef562c1a3451eee2f8601d22%3Ffields%3DcitationCount&amp;query=%24.citationCount&amp;label=citation" alt="Dynamic JSON Badge"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2021-12</td>
<td style="text-align: center;">Retro</td>
<td style="text-align: center;">DeepMind</td>
<td style="text-align: left;"><a href="https://www.deepmind.com/publications/improving-language-models-by-retrieving-from-trillions-of-tokens">Improving language models by retrieving from trillions of tokens</a></td>
<td style="text-align: center;">ICML<br> <img data-src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F002c256d30d6be4b23d365a8de8ae0e67e4c9641%3Ffields%3DcitationCount&amp;query=%24.citationCount&amp;label=citation" alt="Dynamic JSON Badge"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2021-12</td>
<td style="text-align: center;">Gopher</td>
<td style="text-align: center;">DeepMind</td>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2112.11446.pdf">Scaling Language Models: Methods, Analysis &amp; Insights from Training Gopher</a></td>
<td style="text-align: center;"><img data-src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F68f141724814839d556a989646194be88641b143%3Ffields%3DcitationCount&amp;query=%24.citationCount&amp;label=citation" alt="Dynamic JSON Badge"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2022-01</td>
<td style="text-align: center;">COT</td>
<td style="text-align: center;">Google</td>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2201.11903.pdf">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a></td>
<td style="text-align: center;">NeurIPS<br><img data-src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F1b6e810ce0afd0dd093f789d2b2742d047e316d5%3Ffields%3DcitationCount&amp;query=%24.citationCount&amp;label=citation" alt="Dynamic JSON Badge"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2022-01</td>
<td style="text-align: center;">LaMDA</td>
<td style="text-align: center;">Google</td>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2201.08239.pdf">LaMDA: Language Models for Dialog Applications</a></td>
<td style="text-align: center;"><img data-src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fb3848d32f7294ec708627897833c4097eb4d8778%3Ffields%3DcitationCount&amp;query=%24.citationCount&amp;label=citation" alt="Dynamic JSON Badge"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2022-01</td>
<td style="text-align: center;">Minerva</td>
<td style="text-align: center;">Google</td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2206.14858">Solving Quantitative Reasoning Problems with Language Models</a></td>
<td style="text-align: center;">NeurIPS<br> <img data-src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fab0e3d3e4d42369de5933a3b4c237780b41c0d77%3Ffields%3DcitationCount&amp;query=%24.citationCount&amp;label=citation" alt="Dynamic JSON Badge"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2022-01</td>
<td style="text-align: center;">Megatron-Turing NLG</td>
<td style="text-align: center;">Microsoft&amp;NVIDIA</td>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2201.11990.pdf">Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model</a></td>
<td style="text-align: center;"><img data-src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F7cbc2a7843411a1768ab762930707af0a3c33a19%3Ffields%3DcitationCount&amp;query=%24.citationCount&amp;label=citation" alt="Dynamic JSON Badge"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2022-03</td>
<td style="text-align: center;">InstructGPT</td>
<td style="text-align: center;">OpenAI</td>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2203.02155.pdf">Training language models to follow instructions with human feedback</a></td>
<td style="text-align: center;"><img data-src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fd766bffc357127e0dc86dd69561d5aeb520d6f4c%3Ffields%3DcitationCount&amp;query=%24.citationCount&amp;label=citation" alt="Dynamic JSON Badge"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2022-04</td>
<td style="text-align: center;">PaLM</td>
<td style="text-align: center;">Google</td>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2204.02311.pdf">PaLM: Scaling Language Modeling with Pathways</a></td>
<td style="text-align: center;"><img data-src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F094ff971d6a8b8ff870946c9b3ce5aa173617bfb%3Ffields%3DcitationCount&amp;query=%24.citationCount&amp;label=citation" alt="Dynamic JSON Badge"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2022-04</td>
<td style="text-align: center;">Chinchilla</td>
<td style="text-align: center;">DeepMind</td>
<td style="text-align: left;"><a href="https://www.deepmind.com/publications/an-empirical-analysis-of-compute-optimal-large-language-model-training">An empirical analysis of compute-optimal large language model training</a></td>
<td style="text-align: center;">NeurIPS<br> <img data-src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fbb0656031cb17adf6bac5fd0fe8d53dd9c291508%3Ffields%3DcitationCount&amp;query=%24.citationCount&amp;label=citation" alt="Dynamic JSON Badge"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2022-05</td>
<td style="text-align: center;">OPT</td>
<td style="text-align: center;">Meta</td>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2205.01068.pdf">OPT: Open Pre-trained Transformer Language Models</a></td>
<td style="text-align: center;"><img data-src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F13a0d8bb38f739990c8cd65a44061c6534f17221%3Ffields%3DcitationCount&amp;query=%24.citationCount&amp;label=citation" alt="Dynamic JSON Badge"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2022-05</td>
<td style="text-align: center;">UL2</td>
<td style="text-align: center;">Google</td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2205.05131v1">Unifying Language Learning Paradigms</a></td>
<td style="text-align: center;"><img data-src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Ff40aeae3e522ada1f6a9f326841b01ef5c8657b6%3Ffields%3DcitationCount&amp;query=%24.citationCount&amp;label=citation" alt="Dynamic JSON Badge"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2022-06</td>
<td style="text-align: center;">Emergent Abilities</td>
<td style="text-align: center;">Google</td>
<td style="text-align: left;"><a href="https://openreview.net/pdf?id=yzkSU5zdwD">Emergent Abilities of Large Language Models</a></td>
<td style="text-align: center;">TMLR<br><img data-src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fdac3a172b504f4e33c029655e9befb3386e5f63a%3Ffields%3DcitationCount&amp;query=%24.citationCount&amp;label=citation" alt="Dynamic JSON Badge"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2022-06</td>
<td style="text-align: center;">BIG-bench</td>
<td style="text-align: center;">Google</td>
<td style="text-align: left;"><a href="https://github.com/google/BIG-bench">Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models</a></td>
<td style="text-align: center;"><img data-src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F34503c0b6a615124eaf82cb0e4a1dab2866e8980%3Ffields%3DcitationCount&amp;query=%24.citationCount&amp;label=citation" alt="Dynamic JSON Badge"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2022-06</td>
<td style="text-align: center;">METALM</td>
<td style="text-align: center;">Microsoft</td>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2206.06336.pdf">Language Models are General-Purpose Interfaces</a></td>
<td style="text-align: center;"><img data-src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fa8fd9c1625011741f74401ff9bdc1c584e25c86d%3Ffields%3DcitationCount&amp;query=%24.citationCount&amp;label=citation" alt="Dynamic JSON Badge"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2022-09</td>
<td style="text-align: center;">Sparrow</td>
<td style="text-align: center;">DeepMind</td>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2209.14375.pdf">Improving alignment of dialogue agents via targeted human judgements</a></td>
<td style="text-align: center;"><img data-src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F74eae12620bd1c1393e268bddcb6f129a5025166%3Ffields%3DcitationCount&amp;query=%24.citationCount&amp;label=citation" alt="Dynamic JSON Badge"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2022-10</td>
<td style="text-align: center;">Flan-T5/PaLM</td>
<td style="text-align: center;">Google</td>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2210.11416.pdf">Scaling Instruction-Finetuned Language Models</a></td>
<td style="text-align: center;"><img data-src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F5484d228bfc50efbac6e86677bc2ec2ee4ede1a6%3Ffields%3DcitationCount&amp;query=%24.citationCount&amp;label=citation" alt="Dynamic JSON Badge"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2022-10</td>
<td style="text-align: center;">GLM-130B</td>
<td style="text-align: center;">Tsinghua</td>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2210.02414.pdf">GLM-130B: An Open Bilingual Pre-trained Model</a></td>
<td style="text-align: center;">ICLR<br> <img data-src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F1d26c947406173145a4665dd7ab255e03494ea28%3Ffields%3DcitationCount&amp;query=%24.citationCount&amp;label=citation" alt="Dynamic JSON Badge"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2022-11</td>
<td style="text-align: center;">HELM</td>
<td style="text-align: center;">Stanford</td>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2211.09110.pdf">Holistic Evaluation of Language Models</a></td>
<td style="text-align: center;"><img data-src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F5032c0946ee96ff11a292762f23e6377a6cf2731%3Ffields%3DcitationCount&amp;query=%24.citationCount&amp;label=citation" alt="Dynamic JSON Badge"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2022-11</td>
<td style="text-align: center;">BLOOM</td>
<td style="text-align: center;">BigScience</td>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2211.05100.pdf">BLOOM: A 176B-Parameter Open-Access Multilingual Language Model</a></td>
<td style="text-align: center;"><img data-src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F964bd39b546f0f6625ff3b9ef1083f797807ef2e%3Ffields%3DcitationCount&amp;query=%24.citationCount&amp;label=citation" alt="Dynamic JSON Badge"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2022-11</td>
<td style="text-align: center;">Galactica</td>
<td style="text-align: center;">Meta</td>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2211.09085.pdf">Galactica: A Large Language Model for Science</a></td>
<td style="text-align: center;"><img data-src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F7d645a3fd276918374fd9483fd675c28e46506d1%3Ffields%3DcitationCount&amp;query=%24.citationCount&amp;label=citation" alt="Dynamic JSON Badge"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2022-12</td>
<td style="text-align: center;">OPT-IML</td>
<td style="text-align: center;">Meta</td>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2212.12017">OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization</a></td>
<td style="text-align: center;"><img data-src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fe965e93e76a9e6c4e4863d145b5c007b540d575d%3Ffields%3DcitationCount&amp;query=%24.citationCount&amp;label=citation" alt="Dynamic JSON Badge"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2023-01</td>
<td style="text-align: center;">Flan 2022 Collection</td>
<td style="text-align: center;">Google</td>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2301.13688.pdf">The Flan Collection: Designing Data and Methods for Effective Instruction Tuning</a></td>
<td style="text-align: center;"><img data-src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Ff2b0017ddd77fa38760a18145e63553105a1a236%3Ffields%3DcitationCount&amp;query=%24.citationCount&amp;label=citation" alt="Dynamic JSON Badge"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2023-02</td>
<td style="text-align: center;">LLaMA</td>
<td style="text-align: center;">Meta</td>
<td style="text-align: left;"><a href="https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/">LLaMA: Open and Efficient Foundation Language Models</a></td>
<td style="text-align: center;"><img data-src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F57e849d0de13ed5f91d086936296721d4ff75a75%3Ffields%3DcitationCount&amp;query=%24.citationCount&amp;label=citation" alt="Dynamic JSON Badge"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2023-02</td>
<td style="text-align: center;">Kosmos-1</td>
<td style="text-align: center;">Microsoft</td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2302.14045">Language Is Not All You Need: Aligning Perception with Language Models</a></td>
<td style="text-align: center;"><img data-src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Ffbfef4723d8c8467d7bd523e1d0b703cce0e0f9c%3Ffields%3DcitationCount&amp;query=%24.citationCount&amp;label=citation" alt="Dynamic JSON Badge"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2023-03</td>
<td style="text-align: center;">PaLM-E</td>
<td style="text-align: center;">Google</td>
<td style="text-align: left;"><a href="https://palm-e.github.io">PaLM-E: An Embodied Multimodal Language Model</a></td>
<td style="text-align: center;"><img data-src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F38fe8f324d2162e63a967a9ac6648974fc4c66f3%3Ffields%3DcitationCount&amp;query=%24.citationCount&amp;label=citation" alt="Dynamic JSON Badge"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2023-03</td>
<td style="text-align: center;">GPT 4</td>
<td style="text-align: center;">OpenAI</td>
<td style="text-align: left;"><a href="https://openai.com/research/gpt-4">GPT-4 Technical Report</a></td>
<td style="text-align: center;"><img data-src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F8ca62fdf4c276ea3052dc96dcfd8ee96ca425a48%3Ffields%3DcitationCount&amp;query=%24.citationCount&amp;label=citation" alt="Dynamic JSON Badge"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2023-04</td>
<td style="text-align: center;">Pythia</td>
<td style="text-align: center;">EleutherAI et al.</td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2304.01373">Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling</a></td>
<td style="text-align: center;">ICML<br><img data-src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fbe55e8ec4213868db08f2c3168ae666001bea4b8%3Ffields%3DcitationCount&amp;query=%24.citationCount&amp;label=citation" alt="Dynamic JSON Badge"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2023-05</td>
<td style="text-align: center;">Dromedary</td>
<td style="text-align: center;">CMU et al.</td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2305.03047">Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision</a></td>
<td style="text-align: center;"><img data-src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fe01515c6138bc525f7aec30fc85f2adf028d4156%3Ffields%3DcitationCount&amp;query=%24.citationCount&amp;label=citation" alt="Dynamic JSON Badge"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2023-05</td>
<td style="text-align: center;">PaLM 2</td>
<td style="text-align: center;">Google</td>
<td style="text-align: left;"><a href="https://ai.google/static/documents/palm2techreport.pdf">PaLM 2 Technical Report</a></td>
<td style="text-align: center;"><img data-src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Feccee350691708972370b7a12c2a78ad3bddd159%3Ffields%3DcitationCount&amp;query=%24.citationCount&amp;label=citation" alt="Dynamic JSON Badge"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2023-05</td>
<td style="text-align: center;">RWKV</td>
<td style="text-align: center;">Bo Peng</td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2305.13048">RWKV: Reinventing RNNs for the Transformer Era</a></td>
<td style="text-align: center;"><img data-src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F026b3396a63ed5772329708b7580d633bb86bec9%3Ffields%3DcitationCount&amp;query=%24.citationCount&amp;label=citation" alt="Dynamic JSON Badge"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">2023-05</td>
<td style="text-align: center;">DPO</td>
<td style="text-align: center;">Stanford</td>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2305.18290.pdf">Direct Preference Optimization: Your Language Model is Secretly a Reward Model</a></td>
<td style="text-align: center;"><img data-src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F0d1c76d45afa012ded7ab741194baf142117c495%3Ffields%3DcitationCount&amp;query=%24.citationCount&amp;label=citation" alt="Dynamic JSON Badge"></td>
</tr>
<tr class="even">
<td style="text-align: center;">2023-07</td>
<td style="text-align: center;">LLaMA 2</td>
<td style="text-align: center;">Meta</td>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2307.09288.pdf">Llama 2: Open Foundation and Fine-Tuned Chat Models</a></td>
<td style="text-align: center;"><img data-src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F104b0bb1da562d53cbda87aec79ef6a2827d191a%3Ffields%3DcitationCount&amp;query=%24.citationCount&amp;label=citation" alt="Dynamic JSON Badge"></td>
</tr>
</tbody>
</table>
</div>
</div>
</details>
<div class="footer">
<ol type="1">
<li><a href="https://github.com/Hannibal046/Awesome-LLM/blob/main/README.md"><i class="fa-brands fa-github" aria-label="github"></i> Hannibal046/Awesome-LLM</a> <span class="inline-image"><a href="https://awesome.re"><img data-src="https://awesome.re/badge.svg" alt="Awesome"></a></span></li>
</ol>
</div>
</section>

<section id="life-cycle-of-the-llm" class="title-slide slide level1 center" data-auto-animate="true">
<h1 data-id="quarto-animate-title">Life-Cycle of the LLM</h1>
<div class="quarto-layout-panel">
<div class="quarto-layout-row quarto-layout-valign-center">
<div id="column-one" class="quarto-layout-cell" style="flex-basis: 45.0%;justify-content: center;">
<ol type="1">
<li><p>Data collection + preprocessing</p></li>
<li><p><strong>Pre-training</strong></p>
<ul>
<li>Architecture decisions:<br>
<code>{model_size, hyperparameters,</code><br>
<code>parallelism, lr_schedule, ...}</code></li>
</ul></li>
<li><p>Supervised Fine-Tuning</p>
<ul>
<li>Instruction Tuning</li>
<li>Alignment</li>
</ul></li>
<li><p>Deploy (+ monitor, re-evaluate, etc.)</p></li>
</ol>
</div>
<div id="fig-pretrain-two" class="quarto-figure quarto-figure-center" style="flex-basis: 55.0%;justify-content: center;">
<figure>
<p><img data-src="https://jalammar.github.io/images/gpt3/03-gpt3-training-step-back-prop.gif"></p>
<figcaption>Figure&nbsp;3: <strong>Pre-training</strong>: Virtually all of the compute used during pretraining phase<sup>1</sup>.</figcaption>
</figure>
</div>
</div>
</div>
<aside><ol class="aside-footnotes"><li id="fn2"><p>Figure from <a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></p></li></ol></aside></section>


<section id="life-cycle-of-the-llm-pre-training" class="title-slide slide level1 center" data-auto-animate="true">
<h1 data-id="quarto-animate-title">Life-Cycle of the LLM: Pre-training</h1>

<img data-src="https://jalammar.github.io/images/gpt3/03-gpt3-training-step-back-prop.gif" class="r-stretch quarto-figure-center"><p class="caption">Figure&nbsp;4: <strong>Pre-training</strong>: Virtually all of the compute used during pretraining phase</p></section>

<section id="life-cycle-of-the-llm-fine-tuning" class="title-slide slide level1 center" data-auto-animate="true">
<h1 data-id="quarto-animate-title">Life-Cycle of the LLM: Fine-Tuning</h1>

<img data-src="https://jalammar.github.io/images/gpt3/10-gpt3-fine-tuning.gif" class="r-stretch quarto-figure-center"><p class="caption">Figure&nbsp;5: <strong>Fine-tuning</strong>: Fine-tuning actually updates the model’s weights to make the model better at a certain task.</p></section>

<section id="transformer-architecture" class="title-slide slide level1 centeredslide center" height="100%" style="height:100%; font-size: 0.8em;">
<h1>Transformer Architecture</h1>
<!-- # {.centeredslide} -->

<img data-src="./assets/diagrams/transformer.svg" class="r-stretch"><p><span class="citation" data-cites="vaswani2017attention">Vaswani et al. (<a href="#/references" role="doc-biblioref" onclick="">2017</a>)</span></p>
</section>

<section id="forward-pass" class="title-slide slide level1 center">
<h1>Forward Pass</h1>
<div id="fig-forward-pass" class="quarto-figure quarto-figure-center">
<figure>
<video data-autoplay="" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/assisted-generation/gif_1_1080p.mov">
</video>
<figcaption>Figure&nbsp;6: Language Model trained for causal language modeling. Video from: <a href="https://huggingface.co/docs/transformers/main/en/llm_tutorial">🤗 Generation with LLMs</a></figcaption>
</figure>
</div>
</section>

<section id="generating-text" class="title-slide slide level1 center">
<h1>Generating Text</h1>
<div id="fig-generating-text" class="quarto-figure quarto-figure-center">
<figure>
<video data-autoplay="" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/assisted-generation/gif_2_1080p.mov">
</video>
<figcaption>Figure&nbsp;7: Language Model trained for causal language modeling. Video from: <a href="https://huggingface.co/docs/transformers/main/en/llm_tutorial">🤗 Generation with LLMs</a></figcaption>
</figure>
</div>
</section>

<section id="parallelism-overview" class="title-slide slide level1 center">
<h1>Parallelism Overview</h1>
<blockquote>
<p><em><strong>Modern parallelism techniques</strong> enable the training of large language models</em></p>
</blockquote>
</section>

<section id="parallelism-conceptshf-mp" class="title-slide slide level1 center" style="font-size: 0.9em;">
<h1>Parallelism Concepts<sup>1</sup></h1>
<ul>
<li><strong>DataParallel (DP)</strong>:
<ul>
<li><p>The same setup is replicated multiple times, and each being fed a slice of the data.</p></li>
<li><p>The processing is done in parallel and all setups are synchronized at the end of each training step.</p></li>
</ul></li>
<li><strong>TensorParallel (TP)</strong>:
<ul>
<li>Each tensor is split up into multiple chunks.</li>
<li>So, instead of having the whole tensor reside on a single gpu, each shard of the tensor resides on its designated gpu.
<ul>
<li>During processing each shard gets processed separately and in parallel on different GPUs and the results are synced at the end of the step.</li>
<li>This is what one may call horizontal parallelism, as he splitting happens on horizontal level.</li>
</ul></li>
</ul></li>
</ul>
<aside><ol class="aside-footnotes"><li id="fn3"><p><a href="https://huggingface.co/docs/transformers/v4.15.0/parallelism">🤗 Model Parallelism</a></p></li></ol></aside></section>


<section id="parallelism-conceptshf-mp1" class="title-slide slide level1 center" style="font-size: 0.9em;">
<h1>Parallelism Concepts<sup>1</sup></h1>
<ul>
<li><strong>PipelineParallel (PP)</strong>:
<ul>
<li>Model is split up vertically (layer-level) across multiple GPUs, so that only one or several layers of the model are places on a single gpu.
<ul>
<li>Each gpu processes in parallel different stages of the pipeline and working on a small chunk of the batch.</li>
</ul></li>
</ul></li>
<li><strong>Zero Redundancy Optimizer (ZeRO)</strong>:
<ul>
<li>Also performs sharding of the tensors somewhat similar to TP, except the whole tensor gets reconstructed in time for a forward or backward computation, therefore the model doesn’t need to be modified.</li>
<li>It also supports various offloading techniques to compensate for limited GPU memory.</li>
</ul></li>
<li><strong>Sharded DDP</strong>:
<ul>
<li>Another name for the foundational ZeRO concept as used by various other implementations of ZeRO.</li>
</ul></li>
</ul>
<aside><ol class="aside-footnotes"><li id="fn4"><p><a href="https://huggingface.co/docs/transformers/v4.15.0/parallelism">🤗 Model Parallelism</a></p></li></ol></aside></section>


<section id="data-parallelism" class="title-slide slide level1 center" style="font-size: 0.9em;">
<h1>Data Parallelism</h1>
<ul>
<li><strong>Data Parallelism</strong>:
<ul>
<li>The simplest and most common parallelism technique. Workers maintain <em>identical copies</em> of the <em>complete</em> model and work on a <em>subset of the data</em>.</li>
<li><code>DDP</code> supported in PyTorch native.</li>
</ul></li>
<li>ZeRO Data Parallel
<ul>
<li>ZeRO powered data parallelism is shown below<sup>1</sup></li>
</ul></li>
</ul>
<div style="text-align: center;">
<p><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/parallelism-zero.png" width="75%"></p>
</div>
<aside><ol class="aside-footnotes"><li id="fn5"><p><a href="https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/">Blog Post</a></p></li></ol></aside></section>


<section id="tensor-parallelismefficient-large-scale" class="title-slide slide level1 center">
<h1>Tensor Parallelism<sup>1</sup></h1>
<ul>
<li><p>In <strong>Tensor Paralleism</strong> each GPU processes only a slice of a tensor and only aggregates the full tensor for operations that require the whole thing.</p>
<ul>
<li><p>The main building block of any transformer is a fully connected nn.Linear followed by a nonlinear activation GeLU.</p>
<ul>
<li><code>Y = GeLU(XA)</code>, where X and Y are the input and output vectors, and A is the weight matrix.</li>
</ul></li>
<li><p>If we look at the computation in matrix form, it’s easy to see how the matrix multiplication can be split between multiple GPUs:</p></li>
</ul></li>
</ul>
<aside><ol class="aside-footnotes"><li id="fn6"><p><a href="https://arxiv.org/abs/2104.04473">Efficient Large-Scale Language Model Training on GPU Clusters</a></p></li></ol></aside></section>


<section id="tensor-parallelism" class="title-slide slide level1 center" style="font-size: 0.9em;">
<h1>Tensor Parallelism</h1>
<div style="text-align: center;">
<p><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/parallelism-tp-parallel_gemm.png" width="66%" style="text-align: center;"></p>
</div>
<div class="footer">
<p>This information is based on (the much more in-depth) <a href="https://github.com/huggingface/transformers/issues/10321#issuecomment-783543530">TP Overview</a> by <a href="https://github.com/anton-l">@anton-l</a></p>
</div>
</section>

<section id="d-parallelism" class="title-slide slide level1 center" style="font-size:0.9em;">
<h1>3D Parallelism</h1>
<ul>
<li><code>DP</code> + <code>TP</code> + <code>PP</code> (3D) Parallelism</li>
</ul>
<div id="3dparallel-1" style="text-align:center!important; width:90%;">
<p><img data-src="https://www.microsoft.com/en-us/research/uploads/prod/2020/09/Blog_DeepSpeed3_Figure-1_highres-2048x1230.png"></p>
<p>3D Parallelism illustration. Figure from: <a href="https://www.deepspeed.ai/">https://www.deepspeed.ai/</a></p>
</div>
</section>

<section id="d-parallelism-1" class="title-slide slide level1 center">
<h1>3D Parallelism</h1>
<ul>
<li><code>DP</code> + <code>TP</code> + <code>PP</code> (3D) Parallelism</li>
</ul>
<div id="3dparallel" style="text-align:center!important;">
<p><img data-src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/parallelism-deepspeed-3d.png"></p>
<p>Figure taken from <a href="https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/">3D parallelism: Scaling to trillion-parameter models</a></p>
</div>
</section>

<section id="running-on-alcf" class="title-slide slide level1 center" style="font-size: 0.8em; width:100%;">
<h1>Running on ALCF</h1>
<ul>
<li><p>We’ve provided a virtual environment complete with all dependencies for running<br>
<a href="https://github.com/argonne-lcf/Megatron-DeepSpeed"><i class="fa-brands fa-github" aria-label="github"></i> <code>argonne-lcf/Megatron-DeepSpeed</code></a></p>
<div class="sourceCode" id="cb1"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># navigate to directory</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="va">WORKSHOP_DIR</span><span class="op">=</span><span class="st">"/lus/grand/projects/fallwkshp23/"</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="va">PROJECTS_DIR</span><span class="op">=</span><span class="st">"</span><span class="va">${WORKSHOP_DIR}</span><span class="st">/foremans/locations/polaris/projects/"</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="va">PROJECT_DIR</span><span class="op">=</span><span class="st">"</span><span class="va">${PROJECTS_DIR}</span><span class="st">/argonne-lcf/Megatron-DeepSpeed"</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> <span class="st">"</span><span class="va">${PROJECT_DIR}</span><span class="st">"</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># load conda module and activate venv</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="ex">module</span> load conda/2023-10-04<span class="kw">;</span> <span class="ex">conda</span> activate base</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="bu">source</span> venvs/polaris/2023-10-04/bin/activate</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co"># set runtime environment variables</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">IBV_FORK_SAFE</span><span class="op">=</span>1</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">CUDA_DEVICE_MAX_CONNECTIONS</span><span class="op">=</span>1</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># set environment variables for running</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="va">MODEL_SIZE_KEY</span><span class="op">=</span><span class="st">"GPT1_5B"</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="va">SEQ_LEN</span><span class="op">=</span>1024</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="va">MICRO_BATCH</span><span class="op">=</span>1</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="va">SP_TYPE</span><span class="op">=</span><span class="st">"megatron"</span> </span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co"># launch training</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="ex">./ALCF/train-gpt3.sh</span> </span></code></pre></div></li>
</ul>
</section>

<section id="running-on-alcf-1" class="title-slide slide level1 center" style="font-size: 0.8em;">
<h1>Running on ALCF</h1>
<ul>
<li><p>Executable:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="va">MODEL_SIZE_KEY</span><span class="op">=</span><span class="st">"GPT1_5B"</span> <span class="va">SEQ_LEN</span><span class="op">=</span>1024 <span class="va">MICRO_BATCH</span><span class="op">=</span>1 <span class="va">SP_TYPE</span><span class="op">=</span><span class="st">"megatron"</span> <span class="ex">./ALCF/train-gpt3.sh</span></span></code></pre></div></li>
</ul>
<details open="">
<summary>
<b>Output</b>
</summary>
<div class="sourceCode" id="cb3"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="ex">+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="ex">ALCF_DIR:</span> /lus/grand/projects/fallwkshp23/foremans/locations/polaris/projects/argonne-lcf/Megatron-DeepSpeed/ALCF</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="ex">+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="ex">source-ing</span> /lus/grand/projects/fallwkshp23/foremans/locations/polaris/projects/argonne-lcf/Megatron-DeepSpeed/ALCF/setup.sh</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="ex">Setting</span> up MPI on Polaris from x3210c0s1b0n0</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="ex">++</span> SetupMPI<span class="er">(</span><span class="kw">)</span> <span class="ex">+++++++++++++++++++++++++++++++++</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="ex">Using</span> HOSTFILE: /var/spool/pbs/aux/1126584.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="ex">NHOSTS:</span> 2</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="ex">NGPU_PER_HOST:</span> 4</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="ex">NGPUS:</span> 8</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="ex">+++++++++++++++++++++++++++++++++++++++++++++++</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="ex">Skipping</span> setupThetaGPU<span class="er">(</span><span class="kw">)</span> <span class="ex">on</span> x3210c0s1b0n0</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="ex">Setting</span> up MPI on Polaris from x3210c0s1b0n0</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="ex">USING</span> PYTHON: /lus/grand/projects/fallwkshp23/foremans/locations/polaris/projects/argonne-lcf/Megatron-DeepSpeed/venvs/polaris/2023-10-04/bin/python3</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="ex">[...]</span></span></code></pre></div>
</details>
</section>

<section id="running-on-alcf-2" class="title-slide slide level1 center" style="font-size: 0.8em;">
<h1>Running on ALCF</h1>
<p>Once the text has <em>finally</em> stopped printing, you should see output similar to the following:</p>
<div class="code" style="font-size:0.8em;">
<div class="sourceCode" id="cb4"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="ex">Job</span> started at: 2023-10-11-092906 on x3210c0s1b0n0</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="ex">[...]</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="ex">Writing</span> logs to: /lus/grand/projects/fallwkshp23/foremans/locations/polaris/projects/argonne-lcf/Megatron-DeepSpeed/outputs/gpt_SP_actCkpt_GPT13B_z1_seqlen1024_mp8_pp1_sp1_nl40_hs5120_gb1_mb1</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="ex">to</span> view output: tail <span class="at">-f</span> <span class="va">$(</span><span class="fu">tail</span> <span class="at">-1</span> logfiles<span class="va">)</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="ex">i.e.</span> tail <span class="at">-f</span> /lus/grand/projects/fallwkshp23/foremans/locations/polaris/projects/argonne-lcf/Megatron-DeepSpeed/outputs/gpt_SP_actCkpt_GPT13B_z1_seqlen1024_mp8_pp1_sp1_nl40_hs5120_gb1_mb1/logs/foremans-x3210c0s1b0n0-nhosts2-ngpu8-2023-10-11-092906.log</span></code></pre></div>
</div>
<ul>
<li><p>To watch / view the output:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tail</span> <span class="at">-fn</span> 1000 <span class="va">$(</span><span class="fu">tail</span> <span class="at">-1</span> logfiles<span class="va">)</span> <span class="kw">|</span> <span class="fu">less</span></span></code></pre></div></li>
<li><p>will look like<sup>1</sup>:</p></li>
</ul>
<div class="code" style="font-size:0.8em;">
<div class="sourceCode" id="cb6"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="ex">Job</span> started at: 2023-10-11-092906 on x3210c0s1b0n0</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="ex">Training</span> GPT-3 with GPT13B parameters</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="ex">Writing</span> logs to: /lus/grand/projects/fallwkshp23/foremans/locations/polaris/projects/argonne-lcf/Megatron-DeepSpeed/outputs/gpt_SP_actCkpt_GPT13B_z1_seqlen1024_mp8_pp1_sp1_nl40_hs5120_gb1_mb1</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="ex">to</span> view output: tail <span class="at">-f</span> <span class="va">$(</span><span class="fu">tail</span> <span class="at">-1</span> logfiles<span class="va">)</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="ex">i.e.</span> tail <span class="at">-f</span> /lus/grand/projects/fallwkshp23/foremans/locations/polaris/projects/argonne-lcf/Megatron-DeepSpeed/outputs/gpt_SP_actCkpt_GPT13B_z1_seqlen1024_mp8_pp1_sp1_nl40_hs5120_gb1_mb1/logs/foremans-x3210c0s1b0n0-nhosts2-ngpu8-2023-10-11-092906.log</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="ex">using:</span> /lus/grand/projects/fallwkshp23/foremans/locations/polaris/projects/argonne-lcf/Megatron-DeepSpeed/venvs/polaris/2023-10-04/bin/python3</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="ex">[...]</span></span></code></pre></div>
</div>
<aside><ol class="aside-footnotes"><li id="fn7"><p><a href="https://wandb.ai/l2hmc-qcd/GenSLM-Megatron-DS/runs/1uve3tdk?workspace=user-saforem2">🚀 W&amp;B Run: <code>soft-wave-264</code></a></p></li></ol></aside></section>


<section id="getting-started-at-alcf" class="title-slide slide level1 scrollable center" style="font-size: 0.85em;">
<h1>Getting Started at ALCF</h1>
<ul>
<li><p>We provide below the <strong>details</strong> for installing / getting started on ALCF (Polaris)</p></li>
<li><p>Installation:</p>
<ol type="1">
<li><i class="fa-brands fa-github" aria-label="github"></i> Clone GitHub repo:</li>
</ol>
<div class="sourceCode" id="cb7"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">git</span> clone https://github.com/argonne-lcf/Megatron-DeepSpeed</span></code></pre></div>
<ol start="2" type="1">
<li>Load Conda module:
<ul>
<li><p>Polaris:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="kw">[[</span> <span class="st">"</span><span class="va">$(</span><span class="fu">hostname</span><span class="va">)</span><span class="st">==x3*"</span> <span class="kw">]];</span> <span class="cf">then</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">export</span> <span class="va">MACHINE</span><span class="op">=</span><span class="st">"Polaris"</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">export</span> <span class="va">CONDA_DATE</span><span class="op">=</span><span class="st">"2023-10-04"</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    <span class="ex">module</span> load conda/<span class="va">${CONDA_DATE}</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    <span class="ex">conda</span> activate base</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="cf">fi</span></span></code></pre></div></li>
<li><p>ThetaGPU:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="kw">[[</span> <span class="st">"</span><span class="va">$(</span><span class="fu">hostname</span><span class="va">)</span><span class="st">==theta*"</span> <span class="kw">]];</span> <span class="cf">then</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">export</span> <span class="va">MACHINE</span><span class="op">=</span><span class="st">"ThetaGPU"</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">export</span> <span class="va">CONDA_DATE</span><span class="op">=</span><span class="st">"2023-01-10"</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    <span class="ex">module</span> load conda/<span class="va">${CONDA_DATE}</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    <span class="ex">conda</span> activate base</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="cf">fi</span></span></code></pre></div></li>
</ul></li>
</ol></li>
</ul>
</section>

<section id="getting-started" class="title-slide slide level1 center" style="font-size: 0.9em;">
<h1>Getting Started</h1>
<ol start="3" type="1">
<li><p>Setup virtual environment<sup>1</sup>:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> Megatron-DeepSpeed</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="co"># create a new virtual environment</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="fu">mkdir</span> <span class="at">-p</span> <span class="st">"venvs/</span><span class="va">${MACHINE}</span><span class="st">/</span><span class="va">${CONDA_DATE}</span><span class="st">"</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="ex">python3</span> <span class="at">-m</span>  venv <span class="st">"venvs/</span><span class="va">${MACHINE}</span><span class="st">/</span><span class="va">${CONDA_DATE}</span><span class="st">"</span> <span class="at">--system-site-packages</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="bu">source</span> <span class="st">"venvs/</span><span class="va">${MACHINE}</span><span class="st">/</span><span class="va">${CONDA_DATE}</span><span class="st">/bin/activate"</span></span></code></pre></div></li>
<li><p>Create a new folder where we’ll install dependencies:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mkdir</span> <span class="at">-p</span> <span class="st">"deps/</span><span class="va">${MACHINE}</span><span class="st">"</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> <span class="st">"deps/</span><span class="va">${MACHINE}</span><span class="st">"</span></span></code></pre></div></li>
</ol>
<aside><ol class="aside-footnotes"><li id="fn8"><p><strong>On-top of</strong> the base <code>conda</code> environment (<code>--system-site-packages</code>)</p></li></ol></aside></section>


<section id="install-dependencies" class="title-slide slide level1 centerdedslide center" style="height:100%;" data-auto-animate="true">
<h1 data-id="quarto-animate-title">Install Dependencies</h1>
<div class="panel-tabset" style="font-size: 0.8em; width: 100%!important; height: 100%!important;">
<ul id="tabset-1" class="panel-tabset-tabby"><li><a data-tabby-default="" href="#tabset-1-1"><i class="fa-brands fa-github" aria-label="github"></i> Dao-AILab/flash-attention</a></li><li><a href="#tabset-1-2"><i class="fa-brands fa-github" aria-label="github"></i> saforem2/ezpz</a></li><li><a href="#tabset-1-3"><i class="fa-brands fa-github" aria-label="github"></i> NVIDIA/apex</a></li></ul>
<div class="tab-content" style="font-size: 0.8em; width: 100%!important; height: 100%!important;">
<div id="tabset-1-1">
<ul>
<li><p>The <a href="">new release</a> supports three different implementations of FlashAttention: (<code>v1.0.4</code>, <code>v2.x</code>, <code>triton</code>)</p></li>
<li><p>FlashAttention <code>v2.x</code> may have numerical instability issues. For the best performance, we recommend using FlashAttention + Triton</p></li>
<li><p><a href="https://github.com/Dao-AILab/flash-attention"><i class="fa-brands fa-github" aria-label="github"></i> <code>Dao-AILab/flash-attention</code></a>:</p>
<ul>
<li><p><code>v1.0.4</code>:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python3</span> <span class="at">-m</span> pip install flash-attn==1.0.4</span></code></pre></div></li>
<li><p><code>v2.x</code>:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">git</span> clone https://github.com/Dao-AILab/flash-attention</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> flash-attention</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="ex">python3</span> setup.py install</span></code></pre></div></li>
<li><p><code>openai/triton</code>:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="fu">git</span> clone <span class="at">-b</span> legacy-backend https://github.com/openai/triton</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> triton/python</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="ex">python3</span> <span class="at">-m</span> pip install cmake pybind11</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="ex">python3</span> <span class="at">-m</span> pip install .</span></code></pre></div></li>
</ul></li>
</ul>
</div>
<div id="tabset-1-2">
<div id="ezpz">
<ul>
<li><p><a href="https://github.com/saforem2/ezpz"><i class="fa-brands fa-github" aria-label="github"></i> <code>saforem2/ezpz</code></a></p>
<div class="sourceCode" id="cb15"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python3</span> <span class="at">-m</span> pip install <span class="at">-e</span> <span class="st">"git+https://github.com/saforem2/ezpz.git#egg=ezpz"</span></span></code></pre></div></li>
</ul>
</div>
</div>
<div id="tabset-1-3">
<div class="quarto-layout-panel">
<div class="quarto-layout-row quarto-layout-valign-top">
<div id="column-one" class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<ul>
<li><p><a href="https://github.com/NVIDIA/apex"><i class="fa-brands fa-github" aria-label="github"></i> <code>NVIDIA/apex</code></a></p>
<div class="sourceCode" id="cb16"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="fu">git</span> clone https://github.com/NVIDIA/apex</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> ../apex/</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install <span class="at">-v</span> <span class="dt">\ </span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>  <span class="ex">--disable-pip-version-check</span> <span class="dt">\</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">--no-cache-dir</span> <span class="dt">\</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">--no-build-isolation</span> <span class="dt">\</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">--global-option</span><span class="op">=</span><span class="st">"--cpp_ext"</span> <span class="dt">\</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">--global-option</span><span class="op">=</span><span class="st">"--cuda_ext"</span> <span class="dt">\</span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">-e</span> <span class="dt">\</span></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>  ./</span></code></pre></div></li>
</ul>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong><i class="fa-brands fa-python" aria-label="python"></i> <code>conda/2023-10-04</code></strong></p>
</div>
<div class="callout-content">
<!-- []{style='color:var(--dim-text);'} $\hspace{1pt}$ [Recent Talks]{.dim-text} -->
<p><strong>Note</strong>: <code>apex</code> is <strong>already installed</strong> in the base <code>conda/2023-10-04</code> environment on Polaris.</p>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</section>

<section id="running" class="title-slide slide level1 center">
<h1>Running</h1>
<ul>
<li>The <a href="https://github.com/argonne-lcf/Megatron-DeepSpeed/tree/main/ALCF"><i class="fa-brands fa-github" aria-label="github"></i> <code>ALCF/</code></a> directory contains shell scripts for setting up the environment and specifying options to be used for training.</li>
</ul>
<div class="quarto-layout-panel">
<div class="quarto-layout-row quarto-layout-valign-top">
<div id="column-two" class="quarto-layout-cell" style="flex-basis: 39.0%;justify-content: center;">
<div style="line-height: 1.1em;">
<ul>
<li><i class="fa-solid fa-folder-open" aria-label="folder-open"></i> <a href="https://github.com/argonne-lcf/Megatron-DeepSpeed/tree/main/ALCF"><code>ALCF/</code></a><br>
<code>├──</code> <a href="https://github.com/argonne-lcf/Megatron-DeepSpeed/blob/main/ALCF/models.sh"><code>args.sh</code></a><br>
<code>├──</code> <a href="https://github.com/argonne-lcf/Megatron-DeepSpeed/blob/main/ALCF/launch.sh"><code>launch.sh</code></a><br>
<code>├──</code> <a href="https://github.com/argonne-lcf/Megatron-DeepSpeed/blob/main/ALCF/model.sh"><code>model.sh</code></a><br>
<code>├──</code> <a href="https://github.com/argonne-lcf/Megatron-DeepSpeed/blob/main/ALCF/setup.sh"><code>setup.sh</code></a><br>
<code>├──</code> <a href="https://github.com/argonne-lcf/Megatron-DeepSpeed/blob/main/ALCF/submit-pbs.sh"><code>submit-pbs.sh</code></a><br>
<code>├──</code> <a href="https://github.com/argonne-lcf/Megatron-DeepSpeed/blob/main/ALCF/submit.sh"><code>submit.sh</code></a><br>
<code>└──</code> <a href="https://github.com/argonne-lcf/Megatron-DeepSpeed/blob/main/ALCF/train-gpt3.sh"><code>train-gpt3.sh</code></a></li>
</ul>
</div>
</div>
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 2.6%;justify-content: center;">
<p>&nbsp;</p>
</div>
<div id="column-one" class="quarto-layout-cell" style="flex-basis: 58.4%;justify-content: center;">
<ul>
<li><p>Various options can be specified dynamically at runtime by setting them in your environment, e.g.:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set env. vars to use:</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="va">MODEL_SIZE_KEY</span><span class="op">=</span><span class="st">"GPT25B"</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="va">SEQ_LEN</span><span class="op">=</span>1024</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="va">USE_FLASH_ATTN</span><span class="op">=</span>1</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="va">MICRO_BATCH</span><span class="op">=</span>1</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="va">GAS</span><span class="op">=</span>1</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="va">SP_TYPE</span><span class="op">=</span><span class="st">"megatron"</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="va">ZERO_STAGE</span><span class="op">=</span>1</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Launch training:</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a><span class="ex">./ALCF/train-gpt3.sh</span></span></code></pre></div></li>
</ul>
</div>
</div>
</div>
</section>

<section id="details" class="title-slide slide level1 center" style="font-size: 0.9em;">
<h1>Details</h1>
<p>Explicitly:</p>
<ul>
<li><a href="https://github.com/argonne-lcf/Megatron-DeepSpeed/blob/main/ALCF/train-gpt3.sh"><i class="fa-brands fa-github" aria-label="github"></i> <code>ALCF/train-gpt3.sh</code></a>: <strong>Main entry point for training</strong>. This script will:
<ul>
<li>Source the rest of the required <a href="https://github.com/argonne-lcf/Megatron-DeepSpeed/blob/main/ALCF/"><code>ALCF/*.sh</code></a> scripts below <!-- - Launch `mpiexec <mpiexec-args> python3` --> <!--   [`pretrain_gpt.py`](https://github.com/argonne-lcf/Megatron-DeepSpeed/blob/main/ALCF/pretrain_gpt.py`) --> <!--   `<gpt-args>` --></li>
</ul></li>
<li><a href="https://github.com/argonne-lcf/Megatron-DeepSpeed/blob/main/ALCF/models.sh"><i class="fa-brands fa-github" aria-label="github"></i> <code>ALCF/models.sh</code></a>: Contains some example model architectures for GPT3-style models</li>
<li><a href="https://github.com/argonne-lcf/Megatron-DeepSpeed/blob/main/ALCF/args.sh"><i class="fa-brands fa-github" aria-label="github"></i> <code>ALCF/args.sh</code></a>: Logic for parsing / setting up runtime options for Megatron and DeepSpeed</li>
<li><a href="https://github.com/argonne-lcf/Megatron-DeepSpeed/blob/main/ALCF/args.sh"><i class="fa-brands fa-github" aria-label="github"></i> <code>ALCF/setup.sh</code></a>: Locate and activate virtual environment to be used, ensure MPI variables are set properly</li>
<li><a href="https://github.com/argonne-lcf/Megatron-DeepSpeed/blob/main/ALCF/launch.sh"><i class="fa-brands fa-github" aria-label="github"></i> <code>ALCF/launch.sh</code></a>: Identify available resources and build the command to be executed
<ul>
<li>i.e.&nbsp;figure out how many: <code>{nodes, GPUs per node, GPUs total}</code>, to pass to <code>mpi{run,exec}</code></li>
<li>then, use this to launch <code>mpiexec &lt;mpiexec-args&gt; python3</code> <a href="https://github.com/argonne-lcf/Megatron-DeepSpeed/blob/main/ALCF/pretrain_gpt.py%60"><code>pretrain_gpt.py</code></a> <code>&lt;gpt-args&gt;</code></li>
</ul></li>
</ul>
</section>

<section id="deepspeed4science" class="title-slide slide level1 center" data-background-color="#000000" height="100%" style="height: 100%!important; font-size: 0.9em;">
<h1><a href="https://deepspeed4science.ai/">DeepSpeed4Science</a></h1>
<ul>
<li><a href="https://deepspeed4science.ai/2023/09/18/model-showcase-genslms/">Long Sequence Support for GenSLM Model</a></li>
</ul>
<div id="ds4sci-logo" style="text-align: center;">
<p><img data-src="https://saforem2.github.io/assets/ds4sci.svg" style="width:80.0%" data-align="center"></p>
</div>
<div id="genslm" style="text-align: center; font-size: 0.8em;">
<p><img src="https://deepspeed4science.ai/wp-content/uploads/2023/09/Figure-8.gif" width="75%" align="center"></p>
<p>Latent space of biologically meaningful properties for SARS-CoV-2 genomes</p>
</div>
</section>

<section id="loooooooooong-sequence-lengths" class="title-slide slide level1 center" style="height:100%; font-size:0.9em;" data-auto-animate="true">
<h1 data-id="quarto-animate-title">Loooooooooong Sequence Lengths</h1>

<img data-src="https://saforem2.github.io/assets/ds4sci.svg" style="width:100.0%" class="r-stretch"><div id="tbl-results">
<table class="table-striped table-hover">
<caption>Table&nbsp;2: Long sequence length support from <a href="https://github.com/microsoft/Megatron-DeepSpeed"><code>microsoft/Megatron-DeepSpeed</code></a></caption>
<thead>
<tr class="header">
<th style="text-align: center;">Sequence Length</th>
<th style="text-align: center;">Old Megatron-DeepSpeed (TFLOPS)</th>
<th style="text-align: center;">New Megatron-DeepSpeed (TFLOPS)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">2k</td>
<td style="text-align: center;"><span style="text-weight:600;">25</span></td>
<td style="text-align: center;"><span style="text-weight:600;">68</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">4k</td>
<td style="text-align: center;"><span style="text-weight:600;">28</span></td>
<td style="text-align: center;"><span style="text-weight:600;">80</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">8k</td>
<td style="text-align: center;"><span class="red-text">OOM</span></td>
<td style="text-align: center;"><span style="text-weight:600;">86</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">16k</td>
<td style="text-align: center;"><span class="red-text">OOM</span></td>
<td style="text-align: center;"><span style="text-weight:600;">92</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">32k</td>
<td style="text-align: center;"><span class="red-text">OOM</span></td>
<td style="text-align: center;"><span style="text-weight:600;">100</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">64k</td>
<td style="text-align: center;"><span class="red-text">OOM</span></td>
<td style="text-align: center;"><span style="text-weight:600;">106</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">128k</td>
<td style="text-align: center;"><span class="red-text">OOM</span></td>
<td style="text-align: center;"><span style="text-weight:600;">119</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">256k</td>
<td style="text-align: center;"><span class="red-text">OOM</span></td>
<td style="text-align: center;"><span style="text-weight:600;">94</span></td>
</tr>
</tbody>
</table>
</div>
</section>

<section id="loooooooooong-sequence-lengths-1" class="title-slide slide level1 center" style="height:100%; font-size:0.8em;" data-auto-animate="true">
<h1 data-id="quarto-animate-title">Loooooooooong Sequence Lengths</h1>
<ul>
<li>Working with <a href="https://github.com/microsoft/DeepSpeed"><i class="fa-brands fa-microsoft" aria-label="microsoft"></i> Microsoft DeepSpeed</a> team to enable longer sequence lengths (context windows) for LLMs<sup>1</sup>
<ul>
<li><a href="https://www.deepspeed.ai/deepspeed4science/">Release: <strong>DeepSpeed4Science Overview and Tutorial</strong></a></li>
</ul></li>
</ul>
<div id="fig-ds4sci" class="quarto-layout-panel" style="text-align:center;">
<figure>
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<p><img data-src="https://saforem2.github.io/assets/ds4sci.svg" style="width:90.0%"> <img data-src="https://saforem2.github.io/qmd/dsblog_files/figure-html/cell-4-output-1.svg" style="width:49.0%" alt="25B"> <img data-src="https://saforem2.github.io/qmd/dsblog_files/figure-html/cell-4-output-2.svg" style="width:49.0%" alt="33B"></p>
</div>
</div>
<p></p><figcaption>Figure&nbsp;8: Maximum (achievable) <code>SEQ_LEN</code> for both <code>25B</code> and <code>33B</code> models <span class="red-text"><span class="math inline">[</span>WIP<span class="math inline">]</span></span></figcaption><p></p>
</figure>
</div>
<div class="footer">
<p><a href="https://github.com/argonne-lcf/Megatron-DeepSpeed"><i class="fa-brands fa-github" aria-label="github"></i> <code>argonne-lcf/Megatron-DeepSpeed</code></a></p>
</div>
<aside><ol class="aside-footnotes"><li id="fn9"><p>The described experiments were performed on 4 NVIDIA DGX A100-40GB nodes, all using TPSIZE=32[^tpsize], connected through 8 HDR InfiniBand (200Gb/s per HDR).↩︎</p></li></ol></aside></section>


<section id="loooooooooong-sequence-lengths-2" class="title-slide slide level1 centeredslide center" style="height:100%; font-size:0.8em;" data-auto-animate="true">
<h1 data-id="quarto-animate-title">Loooooooooong Sequence Lengths</h1>
<ul>
<li>We can evaluate the performance of our model by looking at two different metrics for throughput: <code>samples_per_sec</code> and <code>TFLOPS</code>.
<ul>
<li>Explicitly, we see that we are able to scale up to significantly longer sequences:<br>
(<code>420k / 128k ~ 3.3x</code>) with only a minimal impact on throughput<br>
performance: (<code>81 / 105 ~ 77%</code>)<sup>1</sup>.</li>
</ul></li>
</ul>
<div style="font-size:0.8em;">
<div id="tbl-seqlen">
<table class="table-striped table-hover">
<caption>Table&nbsp;3: Impact on TFLOPS as a function of increasing sequence length. Table from: <a href="https://api.wandb.ai/links/l2hmc-qcd/awklywn7"><code>throughput/TFLOPS</code></a></caption>
<thead>
<tr class="header">
<th style="text-align: center;">Name</th>
<th style="text-align: center;">Sequence Length (k)</th>
<th style="text-align: center;">(<code>seq_len / min_seq_len</code>)</th>
<th style="text-align: center;">TFLOPS</th>
<th style="text-align: center;">TFLOPS (% of peak)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">GPT25B</td>
<td style="text-align: center;">420</td>
<td style="text-align: center;"><span class="blue-text"><strong>3.28125</strong></span></td>
<td style="text-align: center;">81.77225</td>
<td style="text-align: center;"><span class="blue-text"><strong>77.867</strong></span></td>
</tr>
<tr class="even">
<td style="text-align: center;">GPT25B</td>
<td style="text-align: center;">400</td>
<td style="text-align: center;">3.125</td>
<td style="text-align: center;">90.62</td>
<td style="text-align: center;">86.297</td>
</tr>
<tr class="odd">
<td style="text-align: center;">GPT25B</td>
<td style="text-align: center;">360</td>
<td style="text-align: center;">2.8125</td>
<td style="text-align: center;">81.6325</td>
<td style="text-align: center;">77.7348</td>
</tr>
<tr class="even">
<td style="text-align: center;">GPT25B</td>
<td style="text-align: center;">360</td>
<td style="text-align: center;">2.8125</td>
<td style="text-align: center;">82.6824</td>
<td style="text-align: center;">78.7346</td>
</tr>
<tr class="odd">
<td style="text-align: center;">GPT25B</td>
<td style="text-align: center;">192</td>
<td style="text-align: center;">1.5</td>
<td style="text-align: center;">115.8228</td>
<td style="text-align: center;">110.2927</td>
</tr>
<tr class="even">
<td style="text-align: center;">GPT25B</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">106.672</td>
<td style="text-align: center;">101.5788</td>
</tr>
<tr class="odd">
<td style="text-align: center;">GPT25B</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">105.014</td>
<td style="text-align: center;">100.00</td>
</tr>
</tbody>
</table>
</div>
</div>
<aside><ol class="aside-footnotes"><li id="fn10"><p><a href="https://api.wandb.ai/links/l2hmc-qcd/awklywn7"><code>throughput/TFLOPS</code></a></p></li></ol></aside></section>


<section id="links" class="title-slide slide level1 center">
<h1>Links</h1>
<ol type="1">
<li><a href="https://github.com/Hannibal046/Awesome-LLM/blob/main/README.md"><i class="fa-brands fa-github" aria-label="github"></i> Hannibal046/Awesome-LLM</a> <span class="inline-image"><a href="https://awesome.re"><img data-src="https://awesome.re/badge.svg" alt="Awesome"></a></span></li>
<li><a href="https://github.com/Mooler0410/LLMsPracticalGuide"><i class="fa-brands fa-github" aria-label="github"></i> Mooler0410/LLMsPracticalGuide</a></li>
<li><a href="https://docs.google.com/presentation/d/1636wKStYdT_yRPbJNrf8MLKpQghuWGDmyHinHhAKeXY/edit#slide=id.g238b2698243_0_734https://docs.google.com/presentation/d/1636wKStYdT_yRPbJNrf8MLKpQghuWGDmyHinHhAKeXY/edit#slide=id.g238b2698243_0_734">Large Language Models (in 2023)</a></li>
<li><a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></li>
<li><a href="https://ig.ft.com/generative-ai/">Generative AI Exists because of the Transformer</a></li>
<li><a href="https://jaykmody.com/blog/gpt-from-scratch/">GPT in 60 Lines of Numpy</a></li>
<li><a href="https://openai.com/research/better-language-models">Better Language Models and their Implications</a><br>
</li>
<li><span class="green-text"><i class="fa-solid fa-flask-vial" aria-label="flask-vial"></i></span> <a href="https://bigscience.notion.site/ebe3760ae1724dcc92f2e6877de0938f?v=2faf85dc00794321be14bc892539dd4f">Progress / Artefacts / Outcomes from 🌸 Bloom BigScience</a></li>
</ol>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Acknowledgements</strong></p>
</div>
<div class="callout-content">
<p>This research used resources of the Argonne Leadership Computing Facility,<br>
which is a DOE Office of Science User Facility supported under Contract DE-AC02-06CH11357.</p>
</div>
</div>
</div>
</section>

<section id="references" class="title-slide slide level1 smaller scrollable">
<h1>References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-vaswani2017attention" class="csl-entry" role="listitem">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. <span>“Attention Is All You Need.”</span> <a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a>.
</div>
<div id="ref-yang2023harnessing" class="csl-entry" role="listitem">
Yang, Jingfeng, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Bing Yin, and Xia Hu. 2023. <span>“Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond.”</span> <a href="https://arxiv.org/abs/2304.13712">https://arxiv.org/abs/2304.13712</a>.
</div>
<div id="ref-yao2023tree" class="csl-entry" role="listitem">
Yao, Shunyu, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. <span>“Tree of Thoughts: Deliberate Problem Solving with Large Language Models.”</span> <a href="https://arxiv.org/abs/2305.10601">https://arxiv.org/abs/2305.10601</a>.
</div>
</div>


<img src="assets/anl.svg" class="slide-logo r-stretch"><div class="footer footer-default">

</div>
</section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="site_libs/revealjs/plugin/search/search.js"></script>
  <script src="site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': false,
'previewLinksAuto': false,
'smaller': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":false},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: true,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: false,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: true,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: false,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: true,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      function tippyHover(el, contentFn) {
        const config = {
          allowHTML: true,
          content: contentFn,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start'
        };
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          return note.innerHTML;
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>